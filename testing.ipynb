{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import MCPStdioTool, MCPStreamableHTTPTool, MCPStdioTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = MCPStreamableHTTPTool(\n",
    "            name=\"Microsoft Learn MCP\",\n",
    "            url=\"https://learn.microsoft.com/api/mcp\",\n",
    "            # we don't require approval for microsoft_docs_search tool calls\n",
    "            # but we do for any other tool\n",
    "            # approval_mode={\"never_require_approval\": [\"microsoft_docs_search\"]},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa369a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2 = MCPStdioTool(  \n",
    "    name=\"filesystem\",  \n",
    "    command=\"npx\",  \n",
    "    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", r\"C:\\Users\\blain\\Documents\"],  \n",
    "    description=\"File system operations\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f02bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool3 = MCPStreamableHTTPTool(\n",
    "            name=\"localhost MCP\",\n",
    "            url=\"http://localhost:1337/mcp\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import httpx\n",
    "import json\n",
    "\n",
    "# Enable debug logging for HTTP requests\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(name)s - %(levelname)s - %(message)s')\n",
    "httpx_logger = logging.getLogger(\"httpx\")\n",
    "httpx_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Also enable openai library logging if available\n",
    "logging.getLogger(\"openai\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Patch httpx to log request bodies\n",
    "original_request = httpx.AsyncClient.request\n",
    "\n",
    "async def logged_request(self, method, url, **kwargs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HTTP {method} Request to: {url}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'json' in kwargs:\n",
    "        print(\"\\nRequest Body (JSON):\")\n",
    "        print(json.dumps(kwargs['json'], indent=2))\n",
    "    elif 'content' in kwargs:\n",
    "        print(\"\\nRequest Body (Content):\")\n",
    "        print(kwargs['content'])\n",
    "    \n",
    "    if 'headers' in kwargs:\n",
    "        print(\"\\nRequest Headers:\")\n",
    "        for key, value in kwargs['headers'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    response = await original_request(self, method, url, **kwargs)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HTTP Response Status: {response.status_code}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "httpx.AsyncClient.request = logged_request\n",
    "\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "\n",
    "# tool = MCPStreamableHTTPTool(\n",
    "#             name=\"localhost MCP\",\n",
    "#             url=\"http://localhost:1337/mcp\",\n",
    "#         )\n",
    "llm = llm = OpenAIChatClient(\n",
    "        api_key=\"ollama\", # Just a placeholder, Ollama doesn't require API key\n",
    "        # base_url=\"http://localhost:11434/v1\",\n",
    "        base_url=\"http://ollama.home/v1\",\n",
    "        model_id=\"gpt-oss:20b\",\n",
    "    )\n",
    "agent = llm.create_agent(\n",
    "        name=\"test_agent\",\n",
    "        instructions=\"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. \"\\\n",
    "            \"You can only respond using the tools available to you. Do not make up tool functionality. The tools will be\" \\\n",
    "                \"Provided to you in the prompt.\",\n",
    "        tools=[tool, tool3],\n",
    "    )\n",
    "\n",
    "query = \"What MCP tools do you have?\"\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\n=== HTTP Request/Response Details Below ===\\n\")\n",
    "result = await agent.run(query)\n",
    "print(f\"\\n=== End of HTTP Details ===\\n\")\n",
    "print(f\"Agent: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from random import randint\n",
    "from typing import Annotated\n",
    "\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "\n",
    "\"\"\"\n",
    "Ollama with OpenAI Chat Client Example\n",
    "\n",
    "This sample demonstrates using Ollama models through OpenAI Chat Client by\n",
    "configuring the base URL to point to your local Ollama server for local AI inference.\n",
    "Ollama allows you to run large language models locally on your machine.\n",
    "\n",
    "Environment Variables:\n",
    "- OLLAMA_ENDPOINT: The base URL for your Ollama server (e.g., \"http://localhost:11434/v1/v1/\")\n",
    "- OLLAMA_MODEL: The model name to use (e.g., \"mistral\", \"llama3.2\", \"phi3\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_weather(\n",
    "    location: Annotated[str, \"The location to get the weather for.\"],\n",
    ") -> str:\n",
    "    \"\"\"Get the weather for a given location.\"\"\"\n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    return f\"The weather in {location} is {conditions[randint(0, 3)]} with a high of {randint(10, 30)}Â°C.\"\n",
    "\n",
    "\n",
    "async def non_streaming_example() -> None:\n",
    "    \"\"\"Example of non-streaming response (get the complete result at once).\"\"\"\n",
    "    print(\"=== Non-streaming Response Example ===\")\n",
    "\n",
    "    agent = llm.create_agent(\n",
    "        name=\"WeatherAgent\",\n",
    "        instructions=\"You are a helpful weather agent.\",\n",
    "        tools=get_weather,\n",
    "    )\n",
    "\n",
    "    query = \"What's the weather like in Seattle?\"\n",
    "    print(f\"User: {query}\")\n",
    "    result = await agent.run(query)\n",
    "    print(f\"Agent: {result}\\n\")\n",
    "\n",
    "\n",
    "async def streaming_example() -> None:\n",
    "    \"\"\"Example of streaming response (get results as they are generated).\"\"\"\n",
    "    print(\"=== Streaming Response Example ===\")\n",
    "\n",
    "    agent = llm.create_agent(\n",
    "        name=\"WeatherAgent\",\n",
    "        instructions=\"You are a helpful weather agent.\",\n",
    "        tools=get_weather,\n",
    "    )\n",
    "\n",
    "    query = \"What's the weather like in Portland?\"\n",
    "    print(f\"User: {query}\")\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "    async for chunk in agent.run_stream(query):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa698bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "await streaming_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
