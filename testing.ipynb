{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import MCPStreamableHTTPTool, MCPStdioTool\n",
    "from agent_framework.openai import OpenAIChatClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70fa7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = MCPStreamableHTTPTool(\n",
    "    name=\"Microsoft Learn MCP\",\n",
    "    url=\"https://learn.microsoft.com/api/mcp\",\n",
    "    # we don't require approval for microsoft_docs_search tool calls\n",
    "    # but we do for any other tool\n",
    "    # approval_mode={\"never_require_approval\": [\"microsoft_docs_search\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa369a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2 = MCPStdioTool(\n",
    "    name=\"filesystem\",\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", r\"C:\\Users\\blain\\Documents\"],\n",
    "    description=\"File system operations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f02bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool3 = MCPStreamableHTTPTool(\n",
    "    name=\"localhost MCP\",\n",
    "    url=\"http://localhost:1337/mcp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \n",
      "\n",
      "=== HTTP Request/Response Details Below ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"initialize\",\n",
      "  \"params\": {\n",
      "    \"protocolVersion\": \"2025-06-18\",\n",
      "    \"capabilities\": {\n",
      "      \"sampling\": {}\n",
      "    },\n",
      "    \"clientInfo\": {\n",
      "      \"name\": \"mcp\",\n",
      "      \"version\": \"0.1.0\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 0\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"initialize\",\n",
      "  \"params\": {\n",
      "    \"protocolVersion\": \"2025-06-18\",\n",
      "    \"capabilities\": {\n",
      "      \"sampling\": {}\n",
      "    },\n",
      "    \"clientInfo\": {\n",
      "      \"name\": \"mcp\",\n",
      "      \"version\": \"0.1.0\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 0\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 0,\n",
      "      \"result\": {\n",
      "        \"protocolVersion\": \"2025-06-18\",\n",
      "        \"capabilities\": {\n",
      "          \"experimental\": {},\n",
      "          \"prompts\": {\n",
      "            \"listChanged\": true\n",
      "          },\n",
      "          \"resources\": {\n",
      "            \"subscribe\": false,\n",
      "            \"listChanged\": true\n",
      "          },\n",
      "          \"tools\": {\n",
      "            \"listChanged\": true\n",
      "          }\n",
      "        },\n",
      "        \"serverInfo\": {\n",
      "          \"name\": \"file_read\",\n",
      "          \"version\": \"2.13.0.2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"notifications/initialized\",\n",
      "  \"jsonrpc\": \"2.0\"\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 0,\n",
      "      \"result\": {\n",
      "        \"protocolVersion\": \"2025-06-18\",\n",
      "        \"capabilities\": {\n",
      "          \"experimental\": {},\n",
      "          \"prompts\": {\n",
      "            \"listChanged\": true\n",
      "          },\n",
      "          \"resources\": {\n",
      "            \"subscribe\": false,\n",
      "            \"listChanged\": true\n",
      "          },\n",
      "          \"tools\": {\n",
      "            \"listChanged\": true\n",
      "          }\n",
      "        },\n",
      "        \"serverInfo\": {\n",
      "          \"name\": \"file_read\",\n",
      "          \"version\": \"2.13.0.2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"notifications/initialized\",\n",
      "  \"jsonrpc\": \"2.0\"\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: GET http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body: <empty>\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 202 Accepted\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 202:\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: GET http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body: <empty>\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 202 Accepted\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 202:\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/list\",\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/list\",\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 1,\n",
      "      \"result\": {\n",
      "        \"tools\": [\n",
      "          {\n",
      "            \"name\": \"list_files\",\n",
      "            \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "            \"inputSchema\": {\n",
      "              \"properties\": {},\n",
      "              \"type\": \"object\"\n",
      "            },\n",
      "            \"outputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"result\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"result\"\n",
      "              ],\n",
      "              \"type\": \"object\",\n",
      "              \"x-fastmcp-wrap-result\": true\n",
      "            },\n",
      "            \"_meta\": {\n",
      "              \"_fastmcp\": {\n",
      "                \"tags\": []\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"read_file\",\n",
      "            \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "            \"inputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"filename\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"filename\"\n",
      "              ],\n",
      "              \"type\": \"object\"\n",
      "            },\n",
      "            \"outputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"result\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"result\"\n",
      "              ],\n",
      "              \"type\": \"object\",\n",
      "              \"x-fastmcp-wrap-result\": true\n",
      "            },\n",
      "            \"_meta\": {\n",
      "              \"_fastmcp\": {\n",
      "                \"tags\": []\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"prompts/list\",\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 2\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 1,\n",
      "      \"result\": {\n",
      "        \"tools\": [\n",
      "          {\n",
      "            \"name\": \"list_files\",\n",
      "            \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "            \"inputSchema\": {\n",
      "              \"properties\": {},\n",
      "              \"type\": \"object\"\n",
      "            },\n",
      "            \"outputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"result\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"result\"\n",
      "              ],\n",
      "              \"type\": \"object\",\n",
      "              \"x-fastmcp-wrap-result\": true\n",
      "            },\n",
      "            \"_meta\": {\n",
      "              \"_fastmcp\": {\n",
      "                \"tags\": []\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"read_file\",\n",
      "            \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "            \"inputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"filename\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"filename\"\n",
      "              ],\n",
      "              \"type\": \"object\"\n",
      "            },\n",
      "            \"outputSchema\": {\n",
      "              \"properties\": {\n",
      "                \"result\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"result\"\n",
      "              ],\n",
      "              \"type\": \"object\",\n",
      "              \"x-fastmcp-wrap-result\": true\n",
      "            },\n",
      "            \"_meta\": {\n",
      "              \"_fastmcp\": {\n",
      "                \"tags\": []\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"prompts/list\",\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 2\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 2,\n",
      "      \"result\": {\n",
      "        \"prompts\": []\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 2,\n",
      "      \"result\": {\n",
      "        \"prompts\": []\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: GET http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: GET http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-846\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"reasoning\": \"We need to check file 1 and file 2 content. In files directory. Let's list them.\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_mo8fuk2b\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"list_files\",\n",
      "              \"arguments\": \"{}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 254,\n",
      "    \"completion_tokens\": 41,\n",
      "    \"total_tokens\": 295\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-846\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"reasoning\": \"We need to check file 1 and file 2 content. In files directory. Let's list them.\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_mo8fuk2b\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"list_files\",\n",
      "              \"arguments\": \"{}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 254,\n",
      "    \"completion_tokens\": 41,\n",
      "    \"total_tokens\": 295\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {}\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 3\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {}\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 3\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 3,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"file1.txt\\nfile10.txt\\nfile2.txt\\nfile3.txt\\nfile4.txt\\nfile5.txt\\nfile6.txt\\nfile7.txt\\nfile8.txt\\nfile9.txt\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"file1.txt\\nfile10.txt\\nfile2.txt\\nfile3.txt\\nfile4.txt\\nfile5.txt\\nfile6.txt\\nfile7.txt\\nfile8.txt\\nfile9.txt\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 3,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"file1.txt\\nfile10.txt\\nfile2.txt\\nfile3.txt\\nfile4.txt\\nfile5.txt\\nfile6.txt\\nfile7.txt\\nfile8.txt\\nfile9.txt\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"file1.txt\\nfile10.txt\\nfile2.txt\\nfile3.txt\\nfile4.txt\\nfile5.txt\\nfile6.txt\\nfile7.txt\\nfile8.txt\\nfile9.txt\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-104\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"reasoning\": \"We have file1.txt and file2.txt. Need to check content. Use read_file.\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_1wo6pw6h\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"read_file\",\n",
      "              \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 331,\n",
      "    \"completion_tokens\": 45,\n",
      "    \"total_tokens\": 376\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"read_file\",\n",
      "    \"arguments\": {\n",
      "      \"filename\": \"file1.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 4\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-104\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"reasoning\": \"We have file1.txt and file2.txt. Need to check content. Use read_file.\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_1wo6pw6h\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"read_file\",\n",
      "              \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 331,\n",
      "    \"completion_tokens\": 45,\n",
      "    \"total_tokens\": 376\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"read_file\",\n",
      "    \"arguments\": {\n",
      "      \"filename\": \"file1.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 4\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 4,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"The quick brown fox jumps over the lazy dog.\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"The quick brown fox jumps over the lazy dog.\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 4,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"The quick brown fox jumps over the lazy dog.\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"The quick brown fox jumps over the lazy dog.\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_1wo6pw6h\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_1wo6pw6h\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"The quick brown fox jumps over the lazy dog.\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_1wo6pw6h\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_1wo6pw6h\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"The quick brown fox jumps over the lazy dog.\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-712\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_fpwf5rsx\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"read_file\",\n",
      "              \"arguments\": \"{\\\"filename\\\":\\\"file2.txt\\\"}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 384,\n",
      "    \"completion_tokens\": 20,\n",
      "    \"total_tokens\": 404\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"read_file\",\n",
      "    \"arguments\": {\n",
      "      \"filename\": \"file2.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 5\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-712\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535253,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_fpwf5rsx\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"read_file\",\n",
      "              \"arguments\": \"{\\\"filename\\\":\\\"file2.txt\\\"}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 384,\n",
      "    \"completion_tokens\": 20,\n",
      "    \"total_tokens\": 404\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://localhost:1337/mcp\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"method\": \"tools/call\",\n",
      "  \"params\": {\n",
      "    \"name\": \"read_file\",\n",
      "    \"arguments\": {\n",
      "      \"filename\": \"file2.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 5\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 5,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"Artificial intelligence is transforming the way we work.\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"Artificial intelligence is transforming the way we work.\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://localhost:1337/mcp \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200:\n",
      "event: message\n",
      "data: (pretty-printed below)\n",
      "    {\n",
      "      \"jsonrpc\": \"2.0\",\n",
      "      \"id\": 5,\n",
      "      \"result\": {\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"Artificial intelligence is transforming the way we work.\"\n",
      "          }\n",
      "        ],\n",
      "        \"structuredContent\": {\n",
      "          \"result\": \"Artificial intelligence is transforming the way we work.\"\n",
      "        },\n",
      "        \"isError\": false\n",
      "      }\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_1wo6pw6h\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_1wo6pw6h\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"The quick brown fox jumps over the lazy dog.\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_fpwf5rsx\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file2.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_fpwf5rsx\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Artificial intelligence is transforming the way we work.\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - 2877695037] Request: POST http://ollama.home/v1/chat/completions\n",
      "\n",
      "[httpx - 2877695037] \tRequest body (JSON):\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. You can only respond using the tools available to you. Do not make up tool functionality. The tools will beProvided to you in the prompt.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"type\": \"text\",\n",
      "          \"text\": \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_mo8fuk2b\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"list_files\",\n",
      "            \"arguments\": \"{}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_mo8fuk2b\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"file1.txt\\\\nfile10.txt\\\\nfile2.txt\\\\nfile3.txt\\\\nfile4.txt\\\\nfile5.txt\\\\nfile6.txt\\\\nfile7.txt\\\\nfile8.txt\\\\nfile9.txt\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_1wo6pw6h\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file1.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_1wo6pw6h\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"The quick brown fox jumps over the lazy dog.\\\"}]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_fpwf5rsx\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"read_file\",\n",
      "            \"arguments\": \"{\\\"filename\\\":\\\"file2.txt\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"tool\",\n",
      "      \"tool_call_id\": \"call_fpwf5rsx\",\n",
      "      \"content\": \"[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Artificial intelligence is transforming the way we work.\\\"}]\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"stream\": false,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"list_files\",\n",
      "        \"description\": \"List all files in the files directory.\\n\\nReturns:\\n    A list of filenames in the files directory.\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {},\n",
      "          \"title\": \"list_files_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"read_file\",\n",
      "        \"description\": \"Read the content of a file from the files directory.\\n\\nArgs:\\n    filename: Name of the file to read (e.g. file1.txt, file2.txt)\",\n",
      "        \"parameters\": {\n",
      "          \"properties\": {\n",
      "            \"filename\": {\n",
      "              \"title\": \"Filename\",\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"filename\"\n",
      "          ],\n",
      "          \"title\": \"read_file_input\",\n",
      "          \"type\": \"object\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-527\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535255,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"**File\\u202f1 \\u2013 `file1.txt`**  \\n- Contains the sentence *\\u201cThe quick brown fox jumps over the lazy dog.\\u201d*  \\n- This text definitely talks about a quick brown fox (and also references a lazy dog, but the primary subject is the fox).\\n\\n**File\\u202f2 \\u2013 `file2.txt`**  \\n- The only content is *\\u201cArtificial intelligence is transforming the way we work.\\u201d*  \\n- This does **not** mention a lazy dog or any dog at all.  \\n\\n**Verdict**\\n\\n| File | Meets Requirement |\\n|------|-------------------|\\n| `file1.txt` | \\u2714\\ufe0f (quick brown fox present) |\\n| `file2.txt` | \\u274c (no mention of a lazy dog) |\\n\\nSo, file\\u202f1 satisfies the \\u201cquick brown fox\\u201d requirement, but file\\u202f2 does not satisfy the \\u201clazy dog\\u201d requirement.\",\n",
      "        \"reasoning\": \"We read file1: \\\"The quick brown fox jumps over the lazy dog.\\\" It's about a quick brown fox. good. file2: content about AI. Not about lazy dog. So we must check that file1 is about quick brown fox and file2 is about lazy dog. File2 fails. We should explain.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 436,\n",
      "    \"completion_tokens\": 256,\n",
      "    \"total_tokens\": 692\n",
      "  }\n",
      "}\n",
      "\n",
      "[httpx - _client] HTTP Request: POST http://ollama.home/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "[httpx - 2877695037] \tResponse 200 (JSON):\n",
      "{\n",
      "  \"id\": \"chatcmpl-527\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762535255,\n",
      "  \"model\": \"gpt-oss:20b\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"**File\\u202f1 \\u2013 `file1.txt`**  \\n- Contains the sentence *\\u201cThe quick brown fox jumps over the lazy dog.\\u201d*  \\n- This text definitely talks about a quick brown fox (and also references a lazy dog, but the primary subject is the fox).\\n\\n**File\\u202f2 \\u2013 `file2.txt`**  \\n- The only content is *\\u201cArtificial intelligence is transforming the way we work.\\u201d*  \\n- This does **not** mention a lazy dog or any dog at all.  \\n\\n**Verdict**\\n\\n| File | Meets Requirement |\\n|------|-------------------|\\n| `file1.txt` | \\u2714\\ufe0f (quick brown fox present) |\\n| `file2.txt` | \\u274c (no mention of a lazy dog) |\\n\\nSo, file\\u202f1 satisfies the \\u201cquick brown fox\\u201d requirement, but file\\u202f2 does not satisfy the \\u201clazy dog\\u201d requirement.\",\n",
      "        \"reasoning\": \"We read file1: \\\"The quick brown fox jumps over the lazy dog.\\\" It's about a quick brown fox. good. file2: content about AI. Not about lazy dog. So we must check that file1 is about quick brown fox and file2 is about lazy dog. File2 fails. We should explain.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 436,\n",
      "    \"completion_tokens\": 256,\n",
      "    \"total_tokens\": 692\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== End of HTTP Details ===\n",
      "\n",
      "Agent: **File1  `file1.txt`**  \n",
      "- Contains the sentence *The quick brown fox jumps over the lazy dog.*  \n",
      "- This text definitely talks about a quick brown fox (and also references a lazy dog, but the primary subject is the fox).\n",
      "\n",
      "**File2  `file2.txt`**  \n",
      "- The only content is *Artificial intelligence is transforming the way we work.*  \n",
      "- This does **not** mention a lazy dog or any dog at all.  \n",
      "\n",
      "**Verdict**\n",
      "\n",
      "| File | Meets Requirement |\n",
      "|------|-------------------|\n",
      "| `file1.txt` |  (quick brown fox present) |\n",
      "| `file2.txt` |  (no mention of a lazy dog) |\n",
      "\n",
      "So, file1 satisfies the quick brown fox requirement, but file2 does not satisfy the lazy dog requirement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import httpx\n",
    "import json\n",
    "from httpx._content import ByteStream\n",
    "\n",
    "original_request = None\n",
    "# Restore any previous request patch from earlier runs\n",
    "if \"original_request\" in globals():\n",
    "    httpx.AsyncClient.request = original_request\n",
    "    del globals()[\"original_request\"]\n",
    "\n",
    "# Reset root handlers and silence everything except our custom httpx logs\n",
    "root_logger = logging.getLogger()\n",
    "for handler in list(root_logger.handlers):\n",
    "    root_logger.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "# Limit output to debug entries from httpx.logged_send_single_request\n",
    "class _HttpxFunctionFilter(logging.Filter):\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        return record.funcName == \"logged_send_single_request\"\n",
    "\n",
    "\n",
    "httpx_logger = logging.getLogger(\"httpx\")\n",
    "httpx_logger.handlers.clear()\n",
    "httpx_logger.setLevel(logging.DEBUG)\n",
    "httpx_logger.propagate = False\n",
    "\n",
    "httpx_handler = logging.StreamHandler()\n",
    "httpx_handler.setLevel(logging.DEBUG)\n",
    "httpx_handler.setFormatter(logging.Formatter(\"\\n[%(name)s - %(module)s] %(message)s\"))\n",
    "# httpx_handler.addFilter(_HttpxFunctionFilter())\n",
    "httpx_logger.addHandler(httpx_handler)\n",
    "\n",
    "\n",
    "# Also enable openai library logging if available\n",
    "# logging.getLogger(\"openai\").setLevel(logging.DEBUG)\n",
    "\n",
    "if not hasattr(httpx.AsyncClient, \"_original_send_single_request\"):\n",
    "    httpx.AsyncClient._original_send_single_request = (\n",
    "        httpx.AsyncClient._send_single_request\n",
    "    )\n",
    "\n",
    "\n",
    "def pretty_print_json_in_text(text: str) -> str:\n",
    "    \"\"\"Try to find and pretty-print JSON in text, including SSE format.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    result_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if this is an SSE data line\n",
    "        if line.startswith(\"data: \"):\n",
    "            json_str = line[6:]  # Remove 'data: ' prefix\n",
    "            try:\n",
    "                json_obj = json.loads(json_str)\n",
    "                pretty_json = json.dumps(json_obj, indent=2)\n",
    "                # Indent each line of the pretty JSON for better formatting\n",
    "                indented_json = \"\\n\".join(\n",
    "                    \"    \" + line for line in pretty_json.split(\"\\n\")\n",
    "                )\n",
    "                result_lines.append(f\"data: \\n{indented_json}\")\n",
    "            except json.JSONDecodeError:\n",
    "                result_lines.append(line)\n",
    "        else:\n",
    "            result_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(result_lines)\n",
    "\n",
    "\n",
    "async def logged_send_single_request(self, request):\n",
    "    logger = logging.getLogger(\"httpx\")\n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        stream = request.stream\n",
    "        if isinstance(stream, ByteStream):\n",
    "            body = stream._stream\n",
    "            logger.debug(\"Request: %s %s\", request.method, request.url)\n",
    "            if body:\n",
    "                try:\n",
    "                    body_str = body.decode(\"utf-8\")\n",
    "                    # Try to parse and pretty-print JSON\n",
    "                    try:\n",
    "                        json_obj = json.loads(body_str)\n",
    "                        pretty_json = json.dumps(json_obj, indent=2)\n",
    "                        logger.debug(\"\\tRequest body (JSON):\\n%s\", pretty_json)\n",
    "                    except json.JSONDecodeError:\n",
    "                        logger.debug(\"\\tRequest body (utf-8): %s\", body_str)\n",
    "                except UnicodeDecodeError:\n",
    "                    logger.debug(\"\\tRequest body (bytes): %s\", body)\n",
    "            else:\n",
    "                logger.debug(\"\\tRequest body: <empty>\")\n",
    "        else:\n",
    "            logger.debug(\n",
    "                \"\\tRequest body not logged (stream type: %s)\", type(stream).__name__\n",
    "            )\n",
    "    response = await httpx.AsyncClient._original_send_single_request(self, request)\n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        # logger.debug(\"Response headers: %s\", dict(response.headers))\n",
    "        response_body = await response.aread()\n",
    "        try:\n",
    "            response_str = response_body.decode(\"utf-8\")\n",
    "            # Try to parse and pretty-print JSON (handles both pure JSON and SSE format)\n",
    "            try:\n",
    "                json_obj = json.loads(response_str)\n",
    "                pretty_json = json.dumps(json_obj, indent=2)\n",
    "                logger.debug(\n",
    "                    f\"\\tResponse {response.status_code} (JSON):\\n{pretty_json}\"\n",
    "                )\n",
    "            except json.JSONDecodeError:\n",
    "                # Might be SSE format or other text\n",
    "                pretty_text = pretty_print_json_in_text(response_str)\n",
    "                logger.debug(f\"\\tResponse {response.status_code}:\\n{pretty_text}\")\n",
    "        except UnicodeDecodeError:\n",
    "            logger.debug(f\"\\tResponse {response.status_code} (bytes): {response_body}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "if not getattr(httpx.AsyncClient, \"_logging_patch_applied\", False):\n",
    "    httpx.AsyncClient._send_single_request = logged_send_single_request\n",
    "    httpx.AsyncClient._logging_patch_applied = True\n",
    "\n",
    "\n",
    "# tool = MCPStreamableHTTPTool(\n",
    "#             name=\"localhost MCP\",\n",
    "#             url=\"http://localhost:1337/mcp\",\n",
    "#         )\n",
    "llm = OpenAIChatClient(\n",
    "    api_key=\"ollama\",  # Just a placeholder, Ollama doesn't require API key\n",
    "    # base_url=\"http://localhost:11434/v1\",\n",
    "    base_url=\"http://ollama.home/v1\",\n",
    "    model_id=\"gpt-oss:20b\",\n",
    ")\n",
    "agent = llm.create_agent(\n",
    "    name=\"test_agent\",\n",
    "    instructions=\"You are a helpful agent. You use Model Context Protocol (MCP) tools to answer user questions. \"\n",
    "    \"You can only respond using the tools available to you. Do not make up tool functionality. The tools will be\"\n",
    "    \"Provided to you in the prompt.\",\n",
    "    tools=[tool3],\n",
    ")\n",
    "\n",
    "query = \"You must check that file 1 is about a quick brown fox and file 2 is about a lazy dog. \"\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\n=== HTTP Request/Response Details Below ===\\n\")\n",
    "result = await agent.run(query)\n",
    "print(\"\\n=== End of HTTP Details ===\\n\")\n",
    "print(f\"Agent: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from typing import Annotated\n",
    "\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "\n",
    "\"\"\"\n",
    "Ollama with OpenAI Chat Client Example\n",
    "\n",
    "This sample demonstrates using Ollama models through OpenAI Chat Client by\n",
    "configuring the base URL to point to your local Ollama server for local AI inference.\n",
    "Ollama allows you to run large language models locally on your machine.\n",
    "\n",
    "Environment Variables:\n",
    "- OLLAMA_ENDPOINT: The base URL for your Ollama server (e.g., \"http://localhost:11434/v1/v1/\")\n",
    "- OLLAMA_MODEL: The model name to use (e.g., \"mistral\", \"llama3.2\", \"phi3\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_weather(\n",
    "    location: Annotated[str, \"The location to get the weather for.\"],\n",
    ") -> str:\n",
    "    \"\"\"Get the weather for a given location.\"\"\"\n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    return f\"The weather in {location} is {conditions[randint(0, 3)]} with a high of {randint(10, 30)}C.\"\n",
    "\n",
    "\n",
    "async def non_streaming_example() -> None:\n",
    "    \"\"\"Example of non-streaming response (get the complete result at once).\"\"\"\n",
    "    print(\"=== Non-streaming Response Example ===\")\n",
    "\n",
    "    agent = llm.create_agent(\n",
    "        name=\"WeatherAgent\",\n",
    "        instructions=\"You are a helpful weather agent.\",\n",
    "        tools=get_weather,\n",
    "    )\n",
    "\n",
    "    query = \"What's the weather like in Seattle?\"\n",
    "    print(f\"User: {query}\")\n",
    "    result = await agent.run(query)\n",
    "    print(f\"Agent: {result}\\n\")\n",
    "\n",
    "\n",
    "async def streaming_example() -> None:\n",
    "    \"\"\"Example of streaming response (get results as they are generated).\"\"\"\n",
    "    print(\"=== Streaming Response Example ===\")\n",
    "\n",
    "    agent = llm.create_agent(\n",
    "        name=\"WeatherAgent\",\n",
    "        instructions=\"You are a helpful weather agent.\",\n",
    "        tools=get_weather,\n",
    "    )\n",
    "\n",
    "    query = \"What's the weather like in Portland?\"\n",
    "    print(f\"User: {query}\")\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "    async for chunk in agent.run_stream(query):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa698bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "await streaming_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
